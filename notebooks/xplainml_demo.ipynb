{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136d590f",
   "metadata": {},
   "source": [
    "# XplainML Demo - Interpretable Machine Learning\n",
    "\n",
    "This notebook demonstrates the capabilities of XplainML, a comprehensive tool for interpretable machine learning on tabular data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#Setup-and-Imports)\n",
    "2. [Data Generation and Preprocessing](#Data-Generation-and-Preprocessing)\n",
    "3. [Model Training and Comparison](#Model-Training-and-Comparison)\n",
    "4. [Model Explanations](#Model-Explanations)\n",
    "5. [Visualizations](#Visualizations)\n",
    "6. [Making Predictions](#Making-Predictions)\n",
    "7. [Advanced Features](#Advanced-Features)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9076d79",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ac0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add backend directory to path\n",
    "backend_dir = os.path.join('..', 'backend')\n",
    "if backend_dir not in sys.path:\n",
    "    sys.path.append(backend_dir)\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# XplainML modules from backend\n",
    "from data_preprocessing import DataPreprocessor, create_sample_data\n",
    "from models import MLModel, ModelTuner, compare_models\n",
    "from prediction import Predictor, create_prediction_report\n",
    "from explainer import ModelExplainer\n",
    "from visualizer import ModelVisualizer\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")\n",
    "print(\"🚀 XplainML Demo Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b66b4f",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Preprocessing\n",
    "\n",
    "Let's start by generating sample data and exploring XplainML's preprocessing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample dataset\n",
    "print(\"📊 Generating sample dataset...\")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Generate sample data\n",
    "sample_df = create_sample_data('../data/demo_data.csv', n_samples=1500)\n",
    "\n",
    "print(f\"Dataset created with shape: {sample_df.shape}\")\n",
    "print(f\"Columns: {list(sample_df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "sample_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(\"🔍 Data Exploration\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n📈 Data Info:\")\n",
    "print(sample_df.info())\n",
    "\n",
    "print(\"\\n📊 Summary Statistics:\")\n",
    "sample_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"❓ Missing Values:\")\n",
    "missing_values = sample_df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "if missing_values.sum() > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_df = sample_df.isnull().sum().reset_index()\n",
    "    missing_df.columns = ['Column', 'Missing_Count']\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    sns.barplot(data=missing_df, x='Column', y='Missing_Count')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✅ No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95593b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor and run full pipeline\n",
    "print(\"🔧 Running Data Preprocessing Pipeline...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Run comprehensive preprocessing\n",
    "data = preprocessor.preprocess_pipeline(\n",
    "    file_path='../data/demo_data.csv',\n",
    "    target_column='target',\n",
    "    test_size=0.2,\n",
    "    missing_strategy='mean',\n",
    "    encoding_type='auto',\n",
    "    scaling_type='standard',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Preprocessing completed!\")\n",
    "print(f\"📋 Task Type: {data['task_type']}\")\n",
    "print(f\"🎯 Target: {data['target_name']}\")\n",
    "print(f\"📊 Features: {len(data['feature_names'])}\")\n",
    "print(f\"🏋️ Training samples: {data['X_train'].shape[0]}\")\n",
    "print(f\"🧪 Test samples: {data['X_test'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74e4db",
   "metadata": {},
   "source": [
    "## 3. Model Training and Comparison\n",
    "\n",
    "Now let's train different models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4658073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single Random Forest model\n",
    "print(\"🤖 Training Random Forest Model...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize and train model\n",
    "rf_model = MLModel('random_forest', data['task_type'])\n",
    "rf_model.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "# Evaluate performance\n",
    "train_metrics = rf_model.evaluate(data['X_train'], data['y_train'])\n",
    "test_metrics = rf_model.evaluate(data['X_test'], data['y_test'])\n",
    "\n",
    "print(f\"\\n📊 Training Metrics:\")\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\n🎯 Test Metrics:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cab0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "print(\"⚖️ Comparing Multiple Models...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Run model comparison\n",
    "comparison_results = compare_models(\n",
    "    data['X_train'], data['y_train'],\n",
    "    data['X_test'], data['y_test'],\n",
    "    task_type=data['task_type']\n",
    ")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, result in comparison_results.items():\n",
    "    if 'test_metrics' in result:\n",
    "        row = {'Model': model_name}\n",
    "        row.update(result['test_metrics'])\n",
    "        comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n📊 Model Comparison Results:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "if data['task_type'] == 'classification':\n",
    "    metric = 'accuracy'\n",
    "    title = 'Model Accuracy Comparison'\n",
    "else:\n",
    "    metric = 'r2'\n",
    "    title = 'Model R² Score Comparison'\n",
    "\n",
    "sns.barplot(data=comparison_df, x='Model', y=metric)\n",
    "plt.title(title)\n",
    "plt.ylabel(metric.title())\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_idx = comparison_df[metric].idxmax()\n",
    "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
    "best_score = comparison_df.loc[best_idx, metric]\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"📊 Best {metric}: {best_score:.4f}\")\n",
    "\n",
    "# Use the best model for further analysis\n",
    "best_model = comparison_results[best_model_name]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a799a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning example\n",
    "print(\"⚙️ Hyperparameter Tuning Example...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize tuner\n",
    "tuner = ModelTuner('random_forest', data['task_type'], search_type='grid')\n",
    "\n",
    "# Perform tuning (using smaller parameter space for demo)\n",
    "tuned_model = tuner.tune(data['X_train'], data['y_train'], cv=3)\n",
    "\n",
    "print(f\"\\n🎯 Best Parameters: {tuner.best_params}\")\n",
    "print(f\"📊 Best CV Score: {tuner.best_score:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "tuned_test_metrics = tuned_model.evaluate(data['X_test'], data['y_test'])\n",
    "print(f\"\\n🧪 Tuned Model Test Metrics:\")\n",
    "for metric, value in tuned_test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae2ad",
   "metadata": {},
   "source": [
    "## 4. Model Explanations\n",
    "\n",
    "Now let's generate comprehensive explanations for our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize explainer\n",
    "print(\"🔍 Initializing Model Explainer...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "explainer = ModelExplainer(\n",
    "    model=best_model,\n",
    "    X_train=data['X_train'],\n",
    "    y_train=data['y_train'],\n",
    "    feature_names=data['feature_names']\n",
    ")\n",
    "\n",
    "print(\"✅ Explainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ab2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate global explanations\n",
    "print(\"🌍 Generating Global Explanations...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "global_explanations = explainer.explain_global(method='all', max_display=15)\n",
    "\n",
    "# Display results\n",
    "for method, explanation in global_explanations.items():\n",
    "    if 'feature_importance' in explanation:\n",
    "        print(f\"\\n📊 {method.upper()} Feature Importance:\")\n",
    "        importance = explanation['feature_importance']\n",
    "        \n",
    "        # Show top 10 features\n",
    "        for i, (feature, score) in enumerate(list(importance.items())[:10]):\n",
    "            print(f\"  {i+1:2d}. {feature:20s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d02ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate local explanations for individual samples\n",
    "print(\"🎯 Generating Local Explanations...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Select a few samples for explanation\n",
    "sample_indices = [0, 1, 2]\n",
    "\n",
    "local_explanations = []\n",
    "for idx in sample_indices:\n",
    "    sample = data['X_test'].iloc[idx]\n",
    "    explanation = explainer.explain_local(sample, method='all', num_features=8)\n",
    "    local_explanations.append(explanation)\n",
    "    \n",
    "    print(f\"\\n🔍 Sample {idx + 1} Explanation:\")\n",
    "    \n",
    "    # Show prediction info\n",
    "    if 'prediction_info' in explanation:\n",
    "        pred_info = explanation['prediction_info']\n",
    "        print(f\"  Prediction: {pred_info['prediction']}\")\n",
    "        \n",
    "        if 'probabilities' in pred_info:\n",
    "            probs = pred_info['probabilities']\n",
    "            print(f\"  Probabilities: {probs}\")\n",
    "    \n",
    "    # Show SHAP contributions\n",
    "    if 'shap' in explanation:\n",
    "        contributions = explanation['shap']['feature_contributions']\n",
    "        print(f\"  Top SHAP Contributions:\")\n",
    "        for i, (feature, contrib) in enumerate(list(contributions.items())[:5]):\n",
    "            direction = \"↗️\" if contrib > 0 else \"↘️\"\n",
    "            print(f\"    {direction} {feature}: {contrib:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare explanation methods\n",
    "print(\"⚖️ Comparing Explanation Methods...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use first test sample\n",
    "sample = data['X_test'].iloc[0]\n",
    "comparison = explainer.compare_explanations(sample, methods=['shap', 'lime'])\n",
    "\n",
    "if 'comparison' in comparison:\n",
    "    for comp_name, comp_data in comparison['comparison'].items():\n",
    "        print(f\"\\n🔄 {comp_name}:\")\n",
    "        print(f\"  Overlap Ratio: {comp_data['overlap_ratio']:.3f}\")\n",
    "        print(f\"  Common Features: {comp_data['common_features']}\")\n",
    "        \n",
    "        if comp_data['unique_to_method1']:\n",
    "            method1 = comp_name.split('_vs_')[0]\n",
    "            print(f\"  Unique to {method1}: {comp_data['unique_to_method1']}\")\n",
    "        \n",
    "        if comp_data['unique_to_method2']:\n",
    "            method2 = comp_name.split('_vs_')[1]\n",
    "            print(f\"  Unique to {method2}: {comp_data['unique_to_method2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d9138",
   "metadata": {},
   "source": [
    "## 5. Visualizations\n",
    "\n",
    "Let's create comprehensive visualizations to understand our model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46174a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "print(\"📊 Initializing Visualizer...\")\n",
    "visualizer = ModelVisualizer(best_model, explainer)\n",
    "\n",
    "print(\"✅ Visualizer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "print(\"📊 Feature Importance Visualization\")\n",
    "\n",
    "# Get feature importance from best model\n",
    "importance = best_model.get_feature_importance()\n",
    "\n",
    "if importance:\n",
    "    # Create matplotlib plot\n",
    "    features = list(importance.keys())[:12]\n",
    "    values = list(importance.values())[:12]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    y_pos = np.arange(len(features))\n",
    "    \n",
    "    plt.barh(y_pos, values[::-1], alpha=0.8)\n",
    "    plt.yticks(y_pos, features[::-1])\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.title('Top 12 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ Feature importance not available for this model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction distribution\n",
    "print(\"📈 Prediction Distribution\")\n",
    "\n",
    "predictions = best_model.predict(data['X_test'])\n",
    "y_true = data['y_test']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "if data['task_type'] == 'classification':\n",
    "    # Classification: bar plots\n",
    "    plt.subplot(1, 3, 1)\n",
    "    unique_pred, counts_pred = np.unique(predictions, return_counts=True)\n",
    "    plt.bar(range(len(unique_pred)), counts_pred, alpha=0.7, label='Predictions')\n",
    "    plt.xticks(range(len(unique_pred)), unique_pred)\n",
    "    plt.title('Prediction Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    unique_true, counts_true = np.unique(y_true, return_counts=True)\n",
    "    plt.bar(range(len(unique_true)), counts_true, alpha=0.7, color='orange', label='True Values')\n",
    "    plt.xticks(range(len(unique_true)), unique_true)\n",
    "    plt.title('True Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.subplot(1, 3, 3)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "else:\n",
    "    # Regression: histograms and scatter plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(predictions, alpha=0.7, bins=20, label='Predictions')\n",
    "    plt.hist(y_true, alpha=0.7, bins=20, label='True Values')\n",
    "    plt.title('Distribution Comparison')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(y_true, predictions, alpha=0.6)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title('Predictions vs True Values')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    residuals = y_true - predictions\n",
    "    plt.scatter(predictions, residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Dependence Plots\n",
    "print(\"📊 Partial Dependence Plots\")\n",
    "\n",
    "# Select top 4 important features for PDP\n",
    "if importance:\n",
    "    top_features = list(importance.keys())[:4]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        \n",
    "        # Simple partial dependence calculation\n",
    "        feature_values = data['X_test'][feature]\n",
    "        \n",
    "        if feature_values.dtype in ['int64', 'float64']:\n",
    "            eval_values = np.linspace(feature_values.min(), feature_values.max(), 20)\n",
    "        else:\n",
    "            eval_values = feature_values.unique()[:10]\n",
    "        \n",
    "        partial_deps = []\n",
    "        base_instance = data['X_test'].iloc[0].copy()\n",
    "        \n",
    "        for val in eval_values:\n",
    "            base_instance[feature] = val\n",
    "            pred = best_model.predict(pd.DataFrame([base_instance]))[0]\n",
    "            partial_deps.append(pred)\n",
    "        \n",
    "        plt.plot(eval_values, partial_deps, 'o-', linewidth=2, markersize=4)\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Prediction')\n",
    "        plt.title(f'Partial Dependence: {feature}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ Feature importance not available for PDP generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc782a",
   "metadata": {},
   "source": [
    "## 6. Making Predictions\n",
    "\n",
    "Let's see how to use our trained model for making predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e58682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor\n",
    "print(\"🎯 Setting up Predictor...\")\n",
    "predictor = Predictor(best_model, preprocessor)\n",
    "\n",
    "print(\"✅ Predictor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"📊 Making Predictions on Test Set...\")\n",
    "\n",
    "prediction_result = predictor.predict(\n",
    "    data['X_test'], \n",
    "    return_probabilities=True, \n",
    "    include_confidence=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Predictions completed for {len(prediction_result['predictions'])} samples\")\n",
    "print(f\"📊 Model type: {prediction_result['model_type']}\")\n",
    "print(f\"🎯 Task type: {prediction_result['task_type']}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\n🔍 Sample Predictions:\")\n",
    "for i in range(5):\n",
    "    pred = prediction_result['predictions'][i]\n",
    "    print(f\"  Sample {i+1}: {pred}\")\n",
    "    \n",
    "    if 'probabilities' in prediction_result:\n",
    "        probs = prediction_result['probabilities'][i]\n",
    "        print(f\"    Probabilities: {probs}\")\n",
    "    \n",
    "    if 'confidence' in prediction_result:\n",
    "        conf = prediction_result['confidence'][i]\n",
    "        print(f\"    Confidence: {conf:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da558b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prediction example\n",
    "print(\"🎯 Single Prediction Example...\")\n",
    "\n",
    "# Create a sample instance\n",
    "sample_instance = data['X_test'].iloc[0].to_dict()\n",
    "\n",
    "print(f\"📋 Input features:\")\n",
    "for feature, value in sample_instance.items():\n",
    "    print(f\"  {feature}: {value}\")\n",
    "\n",
    "# Make single prediction\n",
    "single_result = predictor.predict_single(\n",
    "    sample_instance, \n",
    "    return_probabilities=True, \n",
    "    include_confidence=True\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 Prediction Results:\")\n",
    "print(f\"  Prediction: {single_result['prediction']}\")\n",
    "print(f\"  Model: {single_result['model_type']}\")\n",
    "print(f\"  Task: {single_result['task_type']}\")\n",
    "\n",
    "if 'probabilities' in single_result:\n",
    "    print(f\"  Probabilities: {single_result['probabilities']}\")\n",
    "\n",
    "if 'confidence' in single_result:\n",
    "    print(f\"  Confidence: {single_result['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive prediction report\n",
    "print(\"📋 Generating Prediction Report...\")\n",
    "\n",
    "report = create_prediction_report(\n",
    "    predictor, \n",
    "    data['X_test'], \n",
    "    data['y_test']\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Prediction Summary:\")\n",
    "summary = report['prediction_summary']\n",
    "for key, value in summary.items():\n",
    "    if key != 'prediction_time':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n🎯 Evaluation Metrics:\")\n",
    "if 'evaluation_metrics' in report:\n",
    "    for metric, value in report['evaluation_metrics'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "if 'confidence_stats' in report:\n",
    "    print(f\"\\n🔒 Confidence Statistics:\")\n",
    "    conf_stats = report['confidence_stats']\n",
    "    for key, value in conf_stats.items():\n",
    "        print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d13c0",
   "metadata": {},
   "source": [
    "## 7. Advanced Features\n",
    "\n",
    "Let's explore some advanced features of XplainML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature interactions analysis\n",
    "print(\"🔗 Analyzing Feature Interactions...\")\n",
    "\n",
    "interactions = explainer.analyze_feature_interactions(sample_size=200)\n",
    "\n",
    "if isinstance(interactions, dict) and 'top_interactions' in interactions:\n",
    "    print(f\"\\n🔝 Top Feature Interactions:\")\n",
    "    for i, interaction in enumerate(interactions['top_interactions'][:8]):\n",
    "        feature1 = interaction['feature1']\n",
    "        feature2 = interaction['feature2']\n",
    "        strength = interaction['interaction_strength']\n",
    "        print(f\"  {i+1:2d}. {feature1} ↔ {feature2}: {strength:.4f}\")\n",
    "else:\n",
    "    print(f\"⚠️ {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving and loading\n",
    "print(\"💾 Model Saving and Loading...\")\n",
    "\n",
    "# Save the model\n",
    "model_path = '../data/best_model.pkl'\n",
    "best_model.save_model(model_path)\n",
    "print(f\"✅ Model saved to {model_path}\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = MLModel.load_model(model_path)\n",
    "print(f\"✅ Model loaded from {model_path}\")\n",
    "\n",
    "# Verify loaded model works\n",
    "test_pred_original = best_model.predict(data['X_test'][:5])\n",
    "test_pred_loaded = loaded_model.predict(data['X_test'][:5])\n",
    "\n",
    "print(f\"\\n🔍 Verification:\")\n",
    "print(f\"  Original predictions: {test_pred_original}\")\n",
    "print(f\"  Loaded predictions:   {test_pred_loaded}\")\n",
    "print(f\"  Predictions match: {np.array_equal(test_pred_original, test_pred_loaded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive explanation report\n",
    "print(\"📋 Generating Comprehensive Explanation Report...\")\n",
    "\n",
    "comprehensive_report = explainer.generate_explanation_report(\n",
    "    data['X_test'], \n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Report Summary:\")\n",
    "summary = comprehensive_report['summary']\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n🌍 Global Explanation Methods Available:\")\n",
    "for method in comprehensive_report['global_explanations'].keys():\n",
    "    print(f\"  • {method.upper()}\")\n",
    "\n",
    "print(f\"\\n🎯 Local Explanations Generated: {len(comprehensive_report['sample_explanations'])}\")\n",
    "\n",
    "# Show feature interactions summary\n",
    "if 'feature_interactions' in comprehensive_report:\n",
    "    interactions = comprehensive_report['feature_interactions']\n",
    "    if isinstance(interactions, dict) and 'top_interactions' in interactions:\n",
    "        print(f\"\\n🔗 Feature Interactions Analyzed: {len(interactions['top_interactions'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952046f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"🏆 XplainML Demo Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"📊 Dataset:\")\n",
    "print(f\"  • Samples: {data['original_shape'][0]}\")\n",
    "print(f\"  • Features: {len(data['feature_names'])}\")\n",
    "print(f\"  • Task: {data['task_type'].title()}\")\n",
    "print(f\"  • Target: {data['target_name']}\")\n",
    "\n",
    "print(f\"\\n🤖 Best Model:\")\n",
    "print(f\"  • Algorithm: {best_model.model_type.title()}\")\n",
    "print(f\"  • Performance: {best_score:.4f} {metric}\")\n",
    "\n",
    "print(f\"\\n🔍 Explanations Generated:\")\n",
    "print(f\"  • Global methods: {len(global_explanations)}\")\n",
    "print(f\"  • Local samples: {len(local_explanations)}\")\n",
    "print(f\"  • Feature interactions: {'✅' if isinstance(interactions, dict) else '❌'}\")\n",
    "\n",
    "print(f\"\\n📈 Visualizations Created:\")\n",
    "print(f\"  • Feature importance plots\")\n",
    "print(f\"  • Prediction distributions\")\n",
    "print(f\"  • Partial dependence plots\")\n",
    "print(f\"  • Model comparison charts\")\n",
    "\n",
    "print(f\"\\n🎯 Predictions:\")\n",
    "print(f\"  • Test set accuracy: {test_metrics.get('accuracy', test_metrics.get('r2', 'N/A'))}\")\n",
    "print(f\"  • Single predictions: ✅\")\n",
    "print(f\"  • Batch predictions: ✅\")\n",
    "print(f\"  • Confidence scoring: ✅\")\n",
    "\n",
    "print(f\"\\n✨ Advanced Features:\")\n",
    "print(f\"  • Model saving/loading: ✅\")\n",
    "print(f\"  • Hyperparameter tuning: ✅\")\n",
    "print(f\"  • Model comparison: ✅\")\n",
    "print(f\"  • Comprehensive reporting: ✅\")\n",
    "\n",
    "print(f\"\\n🎉 XplainML Demo Completed Successfully!\")\n",
    "print(f\"🚀 Your machine learning models are now interpretable and explainable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc1f4c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the comprehensive capabilities of XplainML:\n",
    "\n",
    "### ✅ What We Accomplished:\n",
    "\n",
    "1. **Data Processing**: Loaded, cleaned, and preprocessed tabular data\n",
    "2. **Model Training**: Trained multiple ML models and compared performance\n",
    "3. **Hyperparameter Tuning**: Optimized model parameters automatically\n",
    "4. **Model Explanations**: Generated global and local explanations using SHAP, LIME, and permutation importance\n",
    "5. **Visualizations**: Created comprehensive plots for understanding model behavior\n",
    "6. **Predictions**: Made single and batch predictions with confidence scores\n",
    "7. **Advanced Features**: Explored feature interactions, model persistence, and comprehensive reporting\n",
    "\n",
    "### 🎯 Key Benefits of XplainML:\n",
    "\n",
    "- **Transparency**: Understand why your model makes specific predictions\n",
    "- **Trust**: Build confidence in your ML models through explanations\n",
    "- **Debugging**: Identify model weaknesses and areas for improvement\n",
    "- **Compliance**: Meet regulatory requirements for explainable AI\n",
    "- **Insights**: Discover important patterns in your data\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "\n",
    "1. Try XplainML with your own datasets\n",
    "2. Explore the web dashboard for interactive analysis\n",
    "3. Use the CLI for automated workflows\n",
    "4. Customize explanations for your specific use case\n",
    "5. Share insights with stakeholders using generated reports\n",
    "\n",
    "---\n",
    "\n",
    "**Happy explaining! 🎉**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
