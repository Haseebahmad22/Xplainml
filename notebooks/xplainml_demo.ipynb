{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136d590f",
   "metadata": {},
   "source": [
    "# XplainML Demo - Interpretable Machine Learning\n",
    "\n",
    "This notebook demonstrates the capabilities of XplainML, a comprehensive tool for interpretable machine learning on tabular data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#Setup-and-Imports)\n",
    "2. [Data Generation and Preprocessing](#Data-Generation-and-Preprocessing)\n",
    "3. [Model Training and Comparison](#Model-Training-and-Comparison)\n",
    "4. [Model Explanations](#Model-Explanations)\n",
    "5. [Visualizations](#Visualizations)\n",
    "6. [Making Predictions](#Making-Predictions)\n",
    "7. [Advanced Features](#Advanced-Features)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9076d79",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ac0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add backend directory to path\n",
    "backend_dir = os.path.join('..', 'backend')\n",
    "if backend_dir not in sys.path:\n",
    "    sys.path.append(backend_dir)\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# XplainML modules from backend\n",
    "from data_preprocessing import DataPreprocessor, create_sample_data\n",
    "from models import MLModel, ModelTuner, compare_models\n",
    "from prediction import Predictor, create_prediction_report\n",
    "from explainer import ModelExplainer\n",
    "from visualizer import ModelVisualizer\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")\n",
    "print(\"üöÄ XplainML Demo Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b66b4f",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Preprocessing\n",
    "\n",
    "Let's start by generating sample data and exploring XplainML's preprocessing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample dataset\n",
    "print(\"üìä Generating sample dataset...\")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Generate sample data\n",
    "sample_df = create_sample_data('../data/demo_data.csv', n_samples=1500)\n",
    "\n",
    "print(f\"Dataset created with shape: {sample_df.shape}\")\n",
    "print(f\"Columns: {list(sample_df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "sample_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(\"üîç Data Exploration\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nüìà Data Info:\")\n",
    "print(sample_df.info())\n",
    "\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "sample_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"‚ùì Missing Values:\")\n",
    "missing_values = sample_df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "if missing_values.sum() > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_df = sample_df.isnull().sum().reset_index()\n",
    "    missing_df.columns = ['Column', 'Missing_Count']\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    sns.barplot(data=missing_df, x='Column', y='Missing_Count')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95593b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor and run full pipeline\n",
    "print(\"üîß Running Data Preprocessing Pipeline...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Run comprehensive preprocessing\n",
    "data = preprocessor.preprocess_pipeline(\n",
    "    file_path='../data/demo_data.csv',\n",
    "    target_column='target',\n",
    "    test_size=0.2,\n",
    "    missing_strategy='mean',\n",
    "    encoding_type='auto',\n",
    "    scaling_type='standard',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing completed!\")\n",
    "print(f\"üìã Task Type: {data['task_type']}\")\n",
    "print(f\"üéØ Target: {data['target_name']}\")\n",
    "print(f\"üìä Features: {len(data['feature_names'])}\")\n",
    "print(f\"üèãÔ∏è Training samples: {data['X_train'].shape[0]}\")\n",
    "print(f\"üß™ Test samples: {data['X_test'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74e4db",
   "metadata": {},
   "source": [
    "## 3. Model Training and Comparison\n",
    "\n",
    "Now let's train different models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4658073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single Random Forest model\n",
    "print(\"ü§ñ Training Random Forest Model...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize and train model\n",
    "rf_model = MLModel('random_forest', data['task_type'])\n",
    "rf_model.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "# Evaluate performance\n",
    "train_metrics = rf_model.evaluate(data['X_train'], data['y_train'])\n",
    "test_metrics = rf_model.evaluate(data['X_test'], data['y_test'])\n",
    "\n",
    "print(f\"\\nüìä Training Metrics:\")\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ Test Metrics:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cab0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "print(\"‚öñÔ∏è Comparing Multiple Models...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Run model comparison\n",
    "comparison_results = compare_models(\n",
    "    data['X_train'], data['y_train'],\n",
    "    data['X_test'], data['y_test'],\n",
    "    task_type=data['task_type']\n",
    ")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, result in comparison_results.items():\n",
    "    if 'test_metrics' in result:\n",
    "        row = {'Model': model_name}\n",
    "        row.update(result['test_metrics'])\n",
    "        comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä Model Comparison Results:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "if data['task_type'] == 'classification':\n",
    "    metric = 'accuracy'\n",
    "    title = 'Model Accuracy Comparison'\n",
    "else:\n",
    "    metric = 'r2'\n",
    "    title = 'Model R¬≤ Score Comparison'\n",
    "\n",
    "sns.barplot(data=comparison_df, x='Model', y=metric)\n",
    "plt.title(title)\n",
    "plt.ylabel(metric.title())\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_idx = comparison_df[metric].idxmax()\n",
    "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
    "best_score = comparison_df.loc[best_idx, metric]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"üìä Best {metric}: {best_score:.4f}\")\n",
    "\n",
    "# Use the best model for further analysis\n",
    "best_model = comparison_results[best_model_name]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a799a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning example\n",
    "print(\"‚öôÔ∏è Hyperparameter Tuning Example...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize tuner\n",
    "tuner = ModelTuner('random_forest', data['task_type'], search_type='grid')\n",
    "\n",
    "# Perform tuning (using smaller parameter space for demo)\n",
    "tuned_model = tuner.tune(data['X_train'], data['y_train'], cv=3)\n",
    "\n",
    "print(f\"\\nüéØ Best Parameters: {tuner.best_params}\")\n",
    "print(f\"üìä Best CV Score: {tuner.best_score:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "tuned_test_metrics = tuned_model.evaluate(data['X_test'], data['y_test'])\n",
    "print(f\"\\nüß™ Tuned Model Test Metrics:\")\n",
    "for metric, value in tuned_test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae2ad",
   "metadata": {},
   "source": [
    "## 4. Model Explanations\n",
    "\n",
    "Now let's generate comprehensive explanations for our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize explainer\n",
    "print(\"üîç Initializing Model Explainer...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "explainer = ModelExplainer(\n",
    "    model=best_model,\n",
    "    X_train=data['X_train'],\n",
    "    y_train=data['y_train'],\n",
    "    feature_names=data['feature_names']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Explainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ab2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate global explanations\n",
    "print(\"üåç Generating Global Explanations...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "global_explanations = explainer.explain_global(method='all', max_display=15)\n",
    "\n",
    "# Display results\n",
    "for method, explanation in global_explanations.items():\n",
    "    if 'feature_importance' in explanation:\n",
    "        print(f\"\\nüìä {method.upper()} Feature Importance:\")\n",
    "        importance = explanation['feature_importance']\n",
    "        \n",
    "        # Show top 10 features\n",
    "        for i, (feature, score) in enumerate(list(importance.items())[:10]):\n",
    "            print(f\"  {i+1:2d}. {feature:20s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d02ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate local explanations for individual samples\n",
    "print(\"üéØ Generating Local Explanations...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Select a few samples for explanation\n",
    "sample_indices = [0, 1, 2]\n",
    "\n",
    "local_explanations = []\n",
    "for idx in sample_indices:\n",
    "    sample = data['X_test'].iloc[idx]\n",
    "    explanation = explainer.explain_local(sample, method='all', num_features=8)\n",
    "    local_explanations.append(explanation)\n",
    "    \n",
    "    print(f\"\\nüîç Sample {idx + 1} Explanation:\")\n",
    "    \n",
    "    # Show prediction info\n",
    "    if 'prediction_info' in explanation:\n",
    "        pred_info = explanation['prediction_info']\n",
    "        print(f\"  Prediction: {pred_info['prediction']}\")\n",
    "        \n",
    "        if 'probabilities' in pred_info:\n",
    "            probs = pred_info['probabilities']\n",
    "            print(f\"  Probabilities: {probs}\")\n",
    "    \n",
    "    # Show SHAP contributions\n",
    "    if 'shap' in explanation:\n",
    "        contributions = explanation['shap']['feature_contributions']\n",
    "        print(f\"  Top SHAP Contributions:\")\n",
    "        for i, (feature, contrib) in enumerate(list(contributions.items())[:5]):\n",
    "            direction = \"‚ÜóÔ∏è\" if contrib > 0 else \"‚ÜòÔ∏è\"\n",
    "            print(f\"    {direction} {feature}: {contrib:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare explanation methods\n",
    "print(\"‚öñÔ∏è Comparing Explanation Methods...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use first test sample\n",
    "sample = data['X_test'].iloc[0]\n",
    "comparison = explainer.compare_explanations(sample, methods=['shap', 'lime'])\n",
    "\n",
    "if 'comparison' in comparison:\n",
    "    for comp_name, comp_data in comparison['comparison'].items():\n",
    "        print(f\"\\nüîÑ {comp_name}:\")\n",
    "        print(f\"  Overlap Ratio: {comp_data['overlap_ratio']:.3f}\")\n",
    "        print(f\"  Common Features: {comp_data['common_features']}\")\n",
    "        \n",
    "        if comp_data['unique_to_method1']:\n",
    "            method1 = comp_name.split('_vs_')[0]\n",
    "            print(f\"  Unique to {method1}: {comp_data['unique_to_method1']}\")\n",
    "        \n",
    "        if comp_data['unique_to_method2']:\n",
    "            method2 = comp_name.split('_vs_')[1]\n",
    "            print(f\"  Unique to {method2}: {comp_data['unique_to_method2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d9138",
   "metadata": {},
   "source": [
    "## 5. Visualizations\n",
    "\n",
    "Let's create comprehensive visualizations to understand our model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46174a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "print(\"üìä Initializing Visualizer...\")\n",
    "visualizer = ModelVisualizer(best_model, explainer)\n",
    "\n",
    "print(\"‚úÖ Visualizer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "print(\"üìä Feature Importance Visualization\")\n",
    "\n",
    "# Get feature importance from best model\n",
    "importance = best_model.get_feature_importance()\n",
    "\n",
    "if importance:\n",
    "    # Create matplotlib plot\n",
    "    features = list(importance.keys())[:12]\n",
    "    values = list(importance.values())[:12]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    y_pos = np.arange(len(features))\n",
    "    \n",
    "    plt.barh(y_pos, values[::-1], alpha=0.8)\n",
    "    plt.yticks(y_pos, features[::-1])\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.title('Top 12 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Feature importance not available for this model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction distribution\n",
    "print(\"üìà Prediction Distribution\")\n",
    "\n",
    "predictions = best_model.predict(data['X_test'])\n",
    "y_true = data['y_test']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "if data['task_type'] == 'classification':\n",
    "    # Classification: bar plots\n",
    "    plt.subplot(1, 3, 1)\n",
    "    unique_pred, counts_pred = np.unique(predictions, return_counts=True)\n",
    "    plt.bar(range(len(unique_pred)), counts_pred, alpha=0.7, label='Predictions')\n",
    "    plt.xticks(range(len(unique_pred)), unique_pred)\n",
    "    plt.title('Prediction Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    unique_true, counts_true = np.unique(y_true, return_counts=True)\n",
    "    plt.bar(range(len(unique_true)), counts_true, alpha=0.7, color='orange', label='True Values')\n",
    "    plt.xticks(range(len(unique_true)), unique_true)\n",
    "    plt.title('True Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.subplot(1, 3, 3)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "else:\n",
    "    # Regression: histograms and scatter plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(predictions, alpha=0.7, bins=20, label='Predictions')\n",
    "    plt.hist(y_true, alpha=0.7, bins=20, label='True Values')\n",
    "    plt.title('Distribution Comparison')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(y_true, predictions, alpha=0.6)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title('Predictions vs True Values')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    residuals = y_true - predictions\n",
    "    plt.scatter(predictions, residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Dependence Plots\n",
    "print(\"üìä Partial Dependence Plots\")\n",
    "\n",
    "# Select top 4 important features for PDP\n",
    "if importance:\n",
    "    top_features = list(importance.keys())[:4]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        \n",
    "        # Simple partial dependence calculation\n",
    "        feature_values = data['X_test'][feature]\n",
    "        \n",
    "        if feature_values.dtype in ['int64', 'float64']:\n",
    "            eval_values = np.linspace(feature_values.min(), feature_values.max(), 20)\n",
    "        else:\n",
    "            eval_values = feature_values.unique()[:10]\n",
    "        \n",
    "        partial_deps = []\n",
    "        base_instance = data['X_test'].iloc[0].copy()\n",
    "        \n",
    "        for val in eval_values:\n",
    "            base_instance[feature] = val\n",
    "            pred = best_model.predict(pd.DataFrame([base_instance]))[0]\n",
    "            partial_deps.append(pred)\n",
    "        \n",
    "        plt.plot(eval_values, partial_deps, 'o-', linewidth=2, markersize=4)\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Prediction')\n",
    "        plt.title(f'Partial Dependence: {feature}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Feature importance not available for PDP generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc782a",
   "metadata": {},
   "source": [
    "## 6. Making Predictions\n",
    "\n",
    "Let's see how to use our trained model for making predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e58682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor\n",
    "print(\"üéØ Setting up Predictor...\")\n",
    "predictor = Predictor(best_model, preprocessor)\n",
    "\n",
    "print(\"‚úÖ Predictor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"üìä Making Predictions on Test Set...\")\n",
    "\n",
    "prediction_result = predictor.predict(\n",
    "    data['X_test'], \n",
    "    return_probabilities=True, \n",
    "    include_confidence=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Predictions completed for {len(prediction_result['predictions'])} samples\")\n",
    "print(f\"üìä Model type: {prediction_result['model_type']}\")\n",
    "print(f\"üéØ Task type: {prediction_result['task_type']}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\nüîç Sample Predictions:\")\n",
    "for i in range(5):\n",
    "    pred = prediction_result['predictions'][i]\n",
    "    print(f\"  Sample {i+1}: {pred}\")\n",
    "    \n",
    "    if 'probabilities' in prediction_result:\n",
    "        probs = prediction_result['probabilities'][i]\n",
    "        print(f\"    Probabilities: {probs}\")\n",
    "    \n",
    "    if 'confidence' in prediction_result:\n",
    "        conf = prediction_result['confidence'][i]\n",
    "        print(f\"    Confidence: {conf:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da558b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prediction example\n",
    "print(\"üéØ Single Prediction Example...\")\n",
    "\n",
    "# Create a sample instance\n",
    "sample_instance = data['X_test'].iloc[0].to_dict()\n",
    "\n",
    "print(f\"üìã Input features:\")\n",
    "for feature, value in sample_instance.items():\n",
    "    print(f\"  {feature}: {value}\")\n",
    "\n",
    "# Make single prediction\n",
    "single_result = predictor.predict_single(\n",
    "    sample_instance, \n",
    "    return_probabilities=True, \n",
    "    include_confidence=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Prediction Results:\")\n",
    "print(f\"  Prediction: {single_result['prediction']}\")\n",
    "print(f\"  Model: {single_result['model_type']}\")\n",
    "print(f\"  Task: {single_result['task_type']}\")\n",
    "\n",
    "if 'probabilities' in single_result:\n",
    "    print(f\"  Probabilities: {single_result['probabilities']}\")\n",
    "\n",
    "if 'confidence' in single_result:\n",
    "    print(f\"  Confidence: {single_result['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive prediction report\n",
    "print(\"üìã Generating Prediction Report...\")\n",
    "\n",
    "report = create_prediction_report(\n",
    "    predictor, \n",
    "    data['X_test'], \n",
    "    data['y_test']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Prediction Summary:\")\n",
    "summary = report['prediction_summary']\n",
    "for key, value in summary.items():\n",
    "    if key != 'prediction_time':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüéØ Evaluation Metrics:\")\n",
    "if 'evaluation_metrics' in report:\n",
    "    for metric, value in report['evaluation_metrics'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "if 'confidence_stats' in report:\n",
    "    print(f\"\\nüîí Confidence Statistics:\")\n",
    "    conf_stats = report['confidence_stats']\n",
    "    for key, value in conf_stats.items():\n",
    "        print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d13c0",
   "metadata": {},
   "source": [
    "## 7. Advanced Features\n",
    "\n",
    "Let's explore some advanced features of XplainML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature interactions analysis\n",
    "print(\"üîó Analyzing Feature Interactions...\")\n",
    "\n",
    "interactions = explainer.analyze_feature_interactions(sample_size=200)\n",
    "\n",
    "if isinstance(interactions, dict) and 'top_interactions' in interactions:\n",
    "    print(f\"\\nüîù Top Feature Interactions:\")\n",
    "    for i, interaction in enumerate(interactions['top_interactions'][:8]):\n",
    "        feature1 = interaction['feature1']\n",
    "        feature2 = interaction['feature2']\n",
    "        strength = interaction['interaction_strength']\n",
    "        print(f\"  {i+1:2d}. {feature1} ‚Üî {feature2}: {strength:.4f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving and loading\n",
    "print(\"üíæ Model Saving and Loading...\")\n",
    "\n",
    "# Save the model\n",
    "model_path = '../data/best_model.pkl'\n",
    "best_model.save_model(model_path)\n",
    "print(f\"‚úÖ Model saved to {model_path}\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = MLModel.load_model(model_path)\n",
    "print(f\"‚úÖ Model loaded from {model_path}\")\n",
    "\n",
    "# Verify loaded model works\n",
    "test_pred_original = best_model.predict(data['X_test'][:5])\n",
    "test_pred_loaded = loaded_model.predict(data['X_test'][:5])\n",
    "\n",
    "print(f\"\\nüîç Verification:\")\n",
    "print(f\"  Original predictions: {test_pred_original}\")\n",
    "print(f\"  Loaded predictions:   {test_pred_loaded}\")\n",
    "print(f\"  Predictions match: {np.array_equal(test_pred_original, test_pred_loaded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive explanation report\n",
    "print(\"üìã Generating Comprehensive Explanation Report...\")\n",
    "\n",
    "comprehensive_report = explainer.generate_explanation_report(\n",
    "    data['X_test'], \n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Report Summary:\")\n",
    "summary = comprehensive_report['summary']\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüåç Global Explanation Methods Available:\")\n",
    "for method in comprehensive_report['global_explanations'].keys():\n",
    "    print(f\"  ‚Ä¢ {method.upper()}\")\n",
    "\n",
    "print(f\"\\nüéØ Local Explanations Generated: {len(comprehensive_report['sample_explanations'])}\")\n",
    "\n",
    "# Show feature interactions summary\n",
    "if 'feature_interactions' in comprehensive_report:\n",
    "    interactions = comprehensive_report['feature_interactions']\n",
    "    if isinstance(interactions, dict) and 'top_interactions' in interactions:\n",
    "        print(f\"\\nüîó Feature Interactions Analyzed: {len(interactions['top_interactions'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952046f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"üèÜ XplainML Demo Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìä Dataset:\")\n",
    "print(f\"  ‚Ä¢ Samples: {data['original_shape'][0]}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(data['feature_names'])}\")\n",
    "print(f\"  ‚Ä¢ Task: {data['task_type'].title()}\")\n",
    "print(f\"  ‚Ä¢ Target: {data['target_name']}\")\n",
    "\n",
    "print(f\"\\nü§ñ Best Model:\")\n",
    "print(f\"  ‚Ä¢ Algorithm: {best_model.model_type.title()}\")\n",
    "print(f\"  ‚Ä¢ Performance: {best_score:.4f} {metric}\")\n",
    "\n",
    "print(f\"\\nüîç Explanations Generated:\")\n",
    "print(f\"  ‚Ä¢ Global methods: {len(global_explanations)}\")\n",
    "print(f\"  ‚Ä¢ Local samples: {len(local_explanations)}\")\n",
    "print(f\"  ‚Ä¢ Feature interactions: {'‚úÖ' if isinstance(interactions, dict) else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\nüìà Visualizations Created:\")\n",
    "print(f\"  ‚Ä¢ Feature importance plots\")\n",
    "print(f\"  ‚Ä¢ Prediction distributions\")\n",
    "print(f\"  ‚Ä¢ Partial dependence plots\")\n",
    "print(f\"  ‚Ä¢ Model comparison charts\")\n",
    "\n",
    "print(f\"\\nüéØ Predictions:\")\n",
    "print(f\"  ‚Ä¢ Test set accuracy: {test_metrics.get('accuracy', test_metrics.get('r2', 'N/A'))}\")\n",
    "print(f\"  ‚Ä¢ Single predictions: ‚úÖ\")\n",
    "print(f\"  ‚Ä¢ Batch predictions: ‚úÖ\")\n",
    "print(f\"  ‚Ä¢ Confidence scoring: ‚úÖ\")\n",
    "\n",
    "print(f\"\\n‚ú® Advanced Features:\")\n",
    "print(f\"  ‚Ä¢ Model saving/loading: ‚úÖ\")\n",
    "print(f\"  ‚Ä¢ Hyperparameter tuning: ‚úÖ\")\n",
    "print(f\"  ‚Ä¢ Model comparison: ‚úÖ\")\n",
    "print(f\"  ‚Ä¢ Comprehensive reporting: ‚úÖ\")\n",
    "\n",
    "print(f\"\\nüéâ XplainML Demo Completed Successfully!\")\n",
    "print(f\"üöÄ Your machine learning models are now interpretable and explainable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc1f4c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the comprehensive capabilities of XplainML:\n",
    "\n",
    "### ‚úÖ What We Accomplished:\n",
    "\n",
    "1. **Data Processing**: Loaded, cleaned, and preprocessed tabular data\n",
    "2. **Model Training**: Trained multiple ML models and compared performance\n",
    "3. **Hyperparameter Tuning**: Optimized model parameters automatically\n",
    "4. **Model Explanations**: Generated global and local explanations using SHAP, LIME, and permutation importance\n",
    "5. **Visualizations**: Created comprehensive plots for understanding model behavior\n",
    "6. **Predictions**: Made single and batch predictions with confidence scores\n",
    "7. **Advanced Features**: Explored feature interactions, model persistence, and comprehensive reporting\n",
    "\n",
    "### üéØ Key Benefits of XplainML:\n",
    "\n",
    "- **Transparency**: Understand why your model makes specific predictions\n",
    "- **Trust**: Build confidence in your ML models through explanations\n",
    "- **Debugging**: Identify model weaknesses and areas for improvement\n",
    "- **Compliance**: Meet regulatory requirements for explainable AI\n",
    "- **Insights**: Discover important patterns in your data\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. Try XplainML with your own datasets\n",
    "2. Explore the web dashboard for interactive analysis\n",
    "3. Use the CLI for automated workflows\n",
    "4. Customize explanations for your specific use case\n",
    "5. Share insights with stakeholders using generated reports\n",
    "\n",
    "---\n",
    "\n",
    "**Happy explaining! üéâ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
